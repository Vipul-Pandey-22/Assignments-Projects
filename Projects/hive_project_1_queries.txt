This is a real time dataset of the ineuron technical consultant team. You have to perform hive 
analysis on this given dataset.
Download Dataset 1 - https://drive.google.com/file/d/1WrG-9qv6atP-W3P_-
gYln1hHyFKRKMHP/view
Download Dataset 2 - https://drive.google.com/file/d/1-JIPCZ34dyN6k9CqJa-Y8yxIGq6vTVXU/view
Note: both files are csv files. 
1. Create a schema based on the given dataset
ans: create table agentperformance
(
sl_no int,
date string,
name string,
total_chats int,
avg_response_time string,
avg_resol_time string,
avg_ratings float,
total_feed int
)
row format delimited
fields terminated by ','
tblproperties("skip.header.line.count"="1");

2. Dump the data inside the hdfs in the given schema location.
ans: load data inpath '/Hive-mini-project/agentperformance.csv' into table agentperformance;

3. List of all agents' names. 
ans: select name from agentperformance;

4. Find out agent average rating.
ans:  select name, avg(avg_ratings) from agentperformance group by name;

5. Total working days for each agent
ans: select name, count(distinct date) as total_working_days from agentperformance group by name;

6. Total query that each agent has taken 
ans: if total_chat column consider as query then ==> select name, sum(total_chats) total_queries from agentperformance group by name limit 50;
	otherwise select name, sum(total_feed) total_queries from agentperformance group by name;
7. Total Feedback that each agent has received
ans: select name, sum(total_feed) from agentperformance group by name;

8. Agent name who have average rating between 3.5 to 4
ans:  select name, avg(avg_ratings) as avg_ratings from agentperformance where avg_ratings between 3.5 and 4 group by name;
 
9. Agent name who have rating less than 3.5 
ans:  select name, avg_ratings from agentperformance where avg_ratings <= 3.5;

10. Agent name who have rating more than 4.5
ans:  select name, avg_ratings from agentperformance where avg_ratings > 4.5;
 
11. How many feedback agents have received more than 4.5 average
ans:  select name, count(total_feed) as num_of_agent_who_have_recieved, avg(avg_ratings) as avg_ratings from agentperformance where avg_ratings > 4.5 group by name;

12. average weekly response time for each agent 
ans: with cte as(
select agent_name,
total_chats,
unix_timestamp(concat(dt," ",avg_rspnse_time),'M/d/yyyy H:mm:ss')-
unix_timestamp(dt,'M/d/yyyy') avg_rspnse_time,
dense_rank() over 
(partition by agent_name
order by weekofyear(from_unixtime((unix_timestamp(dt,'M/d/yyyy')))) ) 
week_no
from agentperformance
)
select agent_name, 
week_no, 
sum(total_chats) total_chats_weekly,
round(avg(avg_rspnse_time),2) weekly_rspnse_time
from cte
group by agent_name, week_no
having total_chats_weekly>0
order by agent_name,week_no;
)
select 
agent_name,
week_no,
if (total_chats_weekly=0, NULL,weekly_rspnse_time) 
from cte1
order by agent_name, week_no

13. average weekly resolution time for each agent
ans:with cte as(
select agent_name,
total_chats,
unix_timestamp(concat(dt," ",avg_resol_time),'M/d/yyyy H:mm:ss')-
unix_timestamp(dt,'M/d/yyyy') avg_resol_time,
dense_rank() over 
(partition by agent_name
order by weekofyear(from_unixtime((unix_timestamp(dt,'M/d/yyyy')))) ) 
week_no
from agentperformance
)
select agent_name, 
week_no, 
sum(total_chats) total_chats_weekly,
round(avg(avg_resol_time),2) weekly_resol_time
from cte
group by agent_name, week_no
having total_chats_weekly>0
order by agent_name,week_no;

14. Find the number of chats on which they have received feedback
ans: select agent_name, sum(total_feed) from agentperformance group by agent_name;

15. Total contribution hour for each and every agentâ€™s weekly basis 
ans: select 
a.agent_name,
dense_rank() over (partition by a.agent_name order by a.weekno) week_no,
round(coalesce(b.hrs,0),4) hour_contribution
from  
(select 
agent_name,
weekofyear(from_unixtime(unix_timestamp(dt,'M/d/yyyy'))) weekno
from agentperformance
group by agent_name, weekofyear(from_unixtime(unix_timestamp(dt,'M/d/yyyy')))) a 
left join 
(
select
agent_name,
weekofyear(from_unixtime(unix_timestamp(dt,'dd-MMM-yy'))) weekno,
sum(cast(split(duration,':')[0] as int) + 
(cast(split(duration,':')[1] as int))/60 + 
(cast(split(duration,':')[2] as int))/3600) hrs
from agentloggingreport
group by agent_name,
weekofyear(from_unixtime(unix_timestamp(dt,'dd-MMM-yy')))
) b
on a.agent_name = b.agent_name 
and a.weekno=b.weekno;

16. Perform inner join, left join and right join based on the agent column and after joining the table 
export that data into your local system.
ans: insert overwrite local directory '/home/cloudera/BIGDATA/innerjoin'
row format delimited
fields terminated by ','
select a.agent_name,b.dt, b.login_time,b.logout_time
from agentperformance a 
inner join 
agentloggingreport b 
on a.agent_name=b.agent_name
and
from_unixtime(unix_timestamp(a.dt, 'M/d/yyyy'),'d-MMM-yy') = b.dt
order by a.agent_name, b.dt;


insert overwrite local directory '/home/cloudera/BIGDATA/leftjoin'
row format delimited
fields terminated by ','
select a.agent_name,b.dt, b.login_time,b.logout_time
from agentperformance a 
left join 
agentloggingreport b 
on a.agent_name=b.agent_name
and
from_unixtime(unix_timestamp(a.dt, 'M/d/yyyy'),'d-MMM-yy') = b.dt
order by a.agent_name, b.dt;

insert overwrite local directory '/home/cloudera/BIGDATA/rightjoin'
row format delimited
fields terminated by ','
select a.agent_name,b.dt, b.login_time,b.logout_time
from agentperformance a 
right join 
agentloggingreport b 
on a.agent_name=b.agent_name
and
from_unixtime(unix_timestamp(a.dt, 'M/d/yyyy'),'d-MMM-yy') = b.dt

17. Perform partitioning on top of the agent column and then on top of that perform bucketing for 
each partitioning.
CREATE TABLE `agentperformance_partbucket`(	
  `srl_no` int, 	
  `dt` date,
  `total_chats` int, 	
  `avg_rspnse_time` int, 	
  `avg_resol_time` int, 	
  `avg_rat` decimal(10,2), 	
  `total_feed` int)
partitioned by (agent_name string)
clustered by (dt) sorted by (dt) into 6 buckets
;
  
set hive.enforce.bucketing = true;
set hive.exec.dynamic.partition=true;    
set hive.exec.dynamic.partition.mode=nonstrict;  


insert into agentperformance_partbucket PARTITION (agent_name)
select
srl_no srl_no,
from_unixtime(unix_timestamp(dt,'M/d/yyyy'),'yyyy-MM-dd') dt  ,
total_chats total_chats ,
cast(split(avg_rspnse_time,':')[0] as int)*3600 + 
(cast(split(avg_rspnse_time,':')[1] as int))*60 + 
(cast(split(avg_rspnse_time,':')[2] as int)) avg_rspnse_time ,
cast(split(avg_resol_time,':')[0] as int)*3600 + 
(cast(split(avg_resol_time,':')[1] as int))*60 + 
(cast(split(avg_resol_time,':')[2] as int)) avg_resol_time  ,
avg_rat  avg_rat,
total_feed total_feed,
agent_name agent_name 
from 
agentperformance
;

select * from agentperformance_partbucket;



create table agentloggingreport_partbucket
(
srl_no int,
dt date,
login_time string,
logout_time string,
duration bigint
)
partitioned by (agent_name string)
clustered by (dt) sorted by  (dt) into 12 buckets;


insert into agentloggingreport_partbucket PARTITION (agent_name)
select
srl_no srl_no,
from_unixtime(unix_timestamp(dt,'dd-MMM-yy'),'yyyy-MM-dd') dt  ,
login_time  login_time,
logout_time  logout_time,
cast(split(duration,':')[0] as int)*3600 + 
(cast(split(duration,':')[1] as int))*60 + 
(cast(split(duration,':')[2] as int)) duration,
agent_name
from 
agentloggingreport
;